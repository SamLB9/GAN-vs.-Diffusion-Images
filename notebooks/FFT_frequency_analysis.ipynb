{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_sg2 = './data/BEST/face/stylegan2/metadata.csv'\n",
    "path_p = './data/BEST/face/palette/metadata.csv'\n",
    "\n",
    "sg2_meta = pd.read_csv(path_sg2)\n",
    "palette_meta = pd.read_csv(path_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sg2_meta = sg2_meta[sg2_meta['category'].str.contains('ffhq', na=False)]\n",
    "print(sg2_meta['category'].unique())          \n",
    "print(sg2_meta['category'].value_counts())\n",
    "sg2_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "palette_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sg2 = './data/BEST/face/stylegan2/metadata.csv'\n",
    "path_p = './data/BEST/face/palette/metadata.csv'\n",
    "sg2_meta = pd.read_csv(path_sg2)\n",
    "palette_meta = pd.read_csv(path_p)\n",
    "sg2_meta = sg2_meta[sg2_meta['category'].str.contains('ffhq', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def azimuthal_average(image, center=None):\n",
    "    \"\"\"\n",
    "    Compute the azimuthally averaged radial profile.\n",
    "    \n",
    "    Parameters:\n",
    "        image (2D np.array): The 2D power spectrum.\n",
    "        center (tuple, optional): The [x, y] center of the image. \n",
    "                                  If None, uses the center of the image.\n",
    "    \n",
    "    Returns:\n",
    "        radial_prof (1D np.array): The azimuthally averaged power spectrum.\n",
    "    \"\"\"\n",
    "    y, x = np.indices(image.shape)\n",
    "    if center is None:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "    # Calculate the distance of every pixel from the center\n",
    "    r = np.hypot(x - center[0], y - center[1])\n",
    "    # Convert distances to integer bins using Python's built-in int\n",
    "    r_int = r.astype(int)\n",
    "    \n",
    "    # Sum and count values in each bin\n",
    "    tbin = np.bincount(r_int.ravel(), image.ravel())\n",
    "    nr = np.bincount(r_int.ravel())\n",
    "    \n",
    "    # Compute the radial average by dividing the summed values by the count in each bin\n",
    "    radial_prof = tbin / (nr + 1e-8)\n",
    "    return radial_prof\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image, convert to grayscale, compute FFT and its azimuthal power spectrum.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): Path to the image file.\n",
    "    \n",
    "    Returns:\n",
    "        radial_prof (1D np.array): Azimuthally averaged power spectrum.\n",
    "    \"\"\"\n",
    "    # Open image and convert to grayscale\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Compute 2D FFT and shift the zero frequency to the center\n",
    "    f = np.fft.fft2(img_array)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    \n",
    "    # Compute the power spectrum (squared magnitude)\n",
    "    ps = np.abs(fshift)**2\n",
    "    \n",
    "    # Compute azimuthally averaged power spectrum\n",
    "    radial_prof = azimuthal_average(ps)\n",
    "    return radial_prof\n",
    "\n",
    "def compute_mean_radial_profile(sample_df):\n",
    "    \"\"\"\n",
    "    Process a DataFrame of image paths, compute each image's radial profile,\n",
    "    and return the average profile over all images.\n",
    "    \n",
    "    Parameters:\n",
    "        sample_df (pd.DataFrame): DataFrame with an 'image_path' column.\n",
    "    \n",
    "    Returns:\n",
    "        mean_profile (1D np.array): Mean azimuthally averaged power spectrum.\n",
    "    \"\"\"\n",
    "    radial_profiles = []\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"File not found: {image_path}\")\n",
    "            continue\n",
    "        radial_prof = process_image(image_path)\n",
    "        radial_profiles.append(radial_prof)\n",
    "    \n",
    "    # Ensure all profiles have the same length by padding shorter arrays with NaN\n",
    "    max_len = max(len(prof) for prof in radial_profiles)\n",
    "    padded_profiles = []\n",
    "    for prof in radial_profiles:\n",
    "        padded = np.full(max_len, np.nan)\n",
    "        padded[:len(prof)] = prof\n",
    "        padded_profiles.append(padded)\n",
    "    \n",
    "    # Compute the mean, ignoring NaN values\n",
    "    mean_profile = np.nanmean(np.array(padded_profiles), axis=0)\n",
    "    return mean_profile\n",
    "\n",
    "# --- Main Workflow ---\n",
    "\n",
    "# Load metadata CSVs for each model\n",
    "path_sg2 = './data/BEST/face/stylegan2/metadata.csv'\n",
    "path_p = './data/BEST/face/palette/metadata.csv'\n",
    "sg2_meta = pd.read_csv(path_sg2)\n",
    "palette_meta = pd.read_csv(path_p)\n",
    "sg2_meta = sg2_meta[sg2_meta['category'].str.contains('ffhq', na=False)]\n",
    "\n",
    "# Modify the image paths by prepending the correct base path\n",
    "palette_meta['image_path'] = './data/BEST/face/palette/' + palette_meta['image_path']\n",
    "sg2_meta['image_path'] = './data/BEST/face/stylegan2/' + sg2_meta['image_path']\n",
    "\n",
    "# Randomly sample 1000 images from each dataset\n",
    "palette_sample = palette_meta.sample(n=1000, random_state=42)\n",
    "sg2_sample = sg2_meta.sample(n=1000, random_state=42)\n",
    "\n",
    "# Compute mean azimuthal power spectrum for each model\n",
    "palette_mean_profile = compute_mean_radial_profile(palette_sample)\n",
    "sg2_mean_profile = compute_mean_radial_profile(sg2_sample)\n",
    "\n",
    "# Plot the average frequency spectra for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(palette_mean_profile, label='Palette')\n",
    "plt.plot(sg2_mean_profile, label='StyleGAN2')\n",
    "plt.xlabel('Frequency bin')\n",
    "plt.ylabel('Power')\n",
    "plt.title('Azimuthally Averaged Power Spectrum Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def azimuthal_average(image, center=None):\n",
    "    \"\"\"\n",
    "    Compute the azimuthally averaged radial profile.\n",
    "    \"\"\"\n",
    "    y, x = np.indices(image.shape)\n",
    "    if center is None:\n",
    "        center = np.array([(x.max()-x.min())/2.0, (y.max()-y.min())/2.0])\n",
    "    # Calculate the distance of every pixel from the center\n",
    "    r = np.hypot(x - center[0], y - center[1])\n",
    "    # Convert distances to integer bins using Python's built-in int\n",
    "    r_int = r.astype(int)\n",
    "    \n",
    "    # Sum and count values in each bin\n",
    "    tbin = np.bincount(r_int.ravel(), image.ravel())\n",
    "    nr = np.bincount(r_int.ravel())\n",
    "    \n",
    "    # Compute the radial average by dividing the summed values by the count in each bin\n",
    "    radial_prof = tbin / (nr + 1e-8)\n",
    "    return radial_prof\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image, convert to grayscale, compute FFT and its azimuthal power spectrum.\n",
    "    \"\"\"\n",
    "    # Open image and convert to grayscale\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Compute 2D FFT and shift the zero frequency to the center\n",
    "    f = np.fft.fft2(img_array)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    \n",
    "    # Compute the power spectrum (squared magnitude)\n",
    "    ps = np.abs(fshift)**2\n",
    "    \n",
    "    # Compute azimuthally averaged power spectrum\n",
    "    radial_prof = azimuthal_average(ps)\n",
    "    return radial_prof\n",
    "\n",
    "def compute_mean_radial_profile(sample_df):\n",
    "    \"\"\"\n",
    "    Process a DataFrame of image paths, compute each image's radial profile,\n",
    "    and return the average profile over all images.\n",
    "    \"\"\"\n",
    "    radial_profiles = []\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"File not found: {image_path}\")\n",
    "            continue\n",
    "        radial_prof = process_image(image_path)\n",
    "        radial_profiles.append(radial_prof)\n",
    "    \n",
    "    # Ensure all profiles have the same length by padding shorter arrays with NaN\n",
    "    max_len = max(len(prof) for prof in radial_profiles)\n",
    "    padded_profiles = []\n",
    "    for prof in radial_profiles:\n",
    "        padded = np.full(max_len, np.nan)\n",
    "        padded[:len(prof)] = prof\n",
    "        padded_profiles.append(padded)\n",
    "    \n",
    "    # Compute the mean, ignoring NaN values\n",
    "    mean_profile = np.nanmean(np.array(padded_profiles), axis=0)\n",
    "    return mean_profile\n",
    "\n",
    "def process_image_2d(image_path):\n",
    "    \"\"\"\n",
    "    Process an image and return its full 2D power spectrum (without azimuthal averaging).\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    img_array = np.array(img)\n",
    "    f = np.fft.fft2(img_array)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    ps = np.abs(fshift)**2\n",
    "    return ps\n",
    "\n",
    "def compute_mean_2d_power(sample_df):\n",
    "    \"\"\"\n",
    "    Compute the average 2D power spectrum for a set of images.\n",
    "    \"\"\"\n",
    "    sum_ps = None\n",
    "    count = 0\n",
    "    for idx, row in sample_df.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"File not found: {image_path}\")\n",
    "            continue\n",
    "        ps = process_image_2d(image_path)\n",
    "        if sum_ps is None:\n",
    "            sum_ps = np.zeros_like(ps, dtype=np.float64)\n",
    "        sum_ps += ps\n",
    "        count += 1\n",
    "    mean_ps = sum_ps / count if count > 0 else None\n",
    "    return mean_ps\n",
    "\n",
    "# --- Main Workflow ---\n",
    "\n",
    "# Load metadata CSVs for each model\n",
    "path_sg2 = './data/BEST/face/stylegan2/metadata.csv'\n",
    "path_p   = './data/BEST/face/palette/metadata.csv'\n",
    "sg2_meta = pd.read_csv(path_sg2)\n",
    "palette_meta = pd.read_csv(path_p)\n",
    "# Filter for a specific category if needed\n",
    "sg2_meta = sg2_meta[sg2_meta['category'].str.contains('ffhq', na=False)]\n",
    "\n",
    "# Modify the image paths by prepending the correct base path\n",
    "palette_meta['image_path'] = './data/BEST/face/palette/' + palette_meta['image_path']\n",
    "sg2_meta['image_path']     = './data/BEST/face/stylegan2/' + sg2_meta['image_path']\n",
    "\n",
    "# Randomly sample 1000 images from each dataset\n",
    "palette_sample = palette_meta.sample(n=1000, random_state=42)\n",
    "sg2_sample = sg2_meta.sample(n=1000, random_state=42)\n",
    "\n",
    "# Compute mean azimuthal power spectrum for each model (1D radial profile)\n",
    "palette_mean_profile = compute_mean_radial_profile(palette_sample)\n",
    "sg2_mean_profile     = compute_mean_radial_profile(sg2_sample)\n",
    "\n",
    "### 1. LOG-LOG PLOT (DC REMOVED)\n",
    "# Remove the first bin (DC component)\n",
    "freq_bins = np.arange(len(palette_mean_profile))[1:]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(freq_bins, palette_mean_profile[1:], label='Palette')\n",
    "plt.plot(freq_bins, sg2_mean_profile[1:], label='StyleGAN2')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Frequency bin (log scale)')\n",
    "plt.ylabel('Power (log scale)')\n",
    "plt.title('Log-Log Azimuthally Averaged Power Spectrum (DC removed)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### 2. PLOT OF LOG10 OF THE RADIAL PROFILE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.log10(palette_mean_profile + 1e-8), label='Palette (log10)')\n",
    "plt.plot(np.log10(sg2_mean_profile + 1e-8), label='StyleGAN2 (log10)')\n",
    "plt.xlabel('Frequency bin')\n",
    "plt.ylabel('Log10(Power)')\n",
    "plt.title('Azimuthally Averaged Power Spectrum (log10 transformed)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### 3. SLOPE ESTIMATION IN LOG-LOG SPACE\n",
    "# Here we fit a line to the log-log data in a chosen frequency range.\n",
    "# Adjust the range as needed.\n",
    "start_idx, end_idx = 5, 50  # Example indices for mid-range frequencies\n",
    "freq = np.arange(start_idx, end_idx)\n",
    "log_freq = np.log10(freq)\n",
    "\n",
    "log_palette = np.log10(palette_mean_profile[start_idx:end_idx] + 1e-8)\n",
    "log_sg2     = np.log10(sg2_mean_profile[start_idx:end_idx] + 1e-8)\n",
    "\n",
    "slope_palette, intercept_palette, r_value, p_value, std_err = linregress(log_freq, log_palette)\n",
    "slope_sg2, intercept_sg2, r_value2, p_value2, std_err2 = linregress(log_freq, log_sg2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(log_freq, log_palette, 'o', label='Palette Data')\n",
    "plt.plot(log_freq, intercept_palette + slope_palette*log_freq, 'r', \n",
    "         label=f'Palette Fit (slope={slope_palette:.2f})')\n",
    "plt.plot(log_freq, log_sg2, 'o', label='StyleGAN2 Data')\n",
    "plt.plot(log_freq, intercept_sg2 + slope_sg2*log_freq, 'g', \n",
    "         label=f'StyleGAN2 Fit (slope={slope_sg2:.2f})')\n",
    "plt.xlabel('Log10(Frequency)')\n",
    "plt.ylabel('Log10(Power)')\n",
    "plt.title('Slope Estimation in Log-Log Space')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### 4. 2D AVERAGE POWER SPECTRUM VISUALIZATION\n",
    "# Compute the average 2D power spectrum for each model.\n",
    "palette_mean_ps = compute_mean_2d_power(palette_sample)\n",
    "sg2_mean_ps     = compute_mean_2d_power(sg2_sample)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.log10(palette_mean_ps + 1e-8), cmap='viridis')\n",
    "plt.colorbar(label='Log10(Power)')\n",
    "plt.title('Palette: Average 2D Power Spectrum')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.log10(sg2_mean_ps + 1e-8), cmap='viridis')\n",
    "plt.colorbar(label='Log10(Power)')\n",
    "plt.title('StyleGAN2: Average 2D Power Spectrum')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from scipy.linalg import sqrtm\n",
    "import lpips\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################################\n",
    "#                             FID IMPLEMENTATION                              #\n",
    "###############################################################################\n",
    "\n",
    "# Define a transformation for the Inception model:\n",
    "# Resize images to 299x299 and normalize according to Inception requirements.\n",
    "fid_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_activations(image_paths, model, batch_size=50):\n",
    "    \"\"\"\n",
    "    Extract activations (features) from the pre-trained model for a list of images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            images = []\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    img = Image.open(path).convert('RGB')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error opening {path}: {e}\")\n",
    "                    continue\n",
    "                img = fid_transform(img)\n",
    "                images.append(img)\n",
    "            if not images:\n",
    "                continue\n",
    "            images = torch.stack(images).to(device)\n",
    "            pred = model(images)\n",
    "            activations.append(pred.cpu().numpy())\n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    return activations\n",
    "\n",
    "# Load pre-trained InceptionV3 model and modify it to output features.\n",
    "inception = models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "# Remove the final fully connected layer by replacing it with an identity function.\n",
    "inception.fc = torch.nn.Identity()\n",
    "\n",
    "def calculate_fid(act1, act2):\n",
    "    \"\"\"\n",
    "    Calculate the Fréchet Inception Distance given two sets of activations.\n",
    "    \"\"\"\n",
    "    mu1 = np.mean(act1, axis=0)\n",
    "    mu2 = np.mean(act2, axis=0)\n",
    "    sigma1 = np.cov(act1, rowvar=False)\n",
    "    sigma2 = np.cov(act2, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Example: Get image paths from your DataFrames (adjust based on your code)\n",
    "# Assume palette_sample and sg2_sample have an 'image_path' column.\n",
    "palette_image_paths = palette_sample['image_path'].tolist()\n",
    "sg2_image_paths = sg2_sample['image_path'].tolist()\n",
    "\n",
    "# Extract activations for both sets.\n",
    "activations_palette = get_activations(palette_image_paths, inception)\n",
    "activations_sg2     = get_activations(sg2_image_paths, inception)\n",
    "\n",
    "# Compute the FID score.\n",
    "fid_value = calculate_fid(activations_palette, activations_sg2)\n",
    "print(\"FID between Palette and StyleGAN2:\", fid_value)\n",
    "\n",
    "###############################################################################\n",
    "#                        LPIPS (PERCEPTUAL METRIC)                            #\n",
    "###############################################################################\n",
    "\n",
    "# Initialize the LPIPS model. You can choose 'alex' or 'vgg' as the backbone.\n",
    "lpips_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "# Define a transform for LPIPS.\n",
    "lpips_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # LPIPS typically works on 256x256 images.\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def compute_lpips_score(image_path1, image_path2):\n",
    "    \"\"\"\n",
    "    Compute the LPIPS score between two images.\n",
    "    LPIPS expects images in the range [-1, 1], so we scale them accordingly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img1 = Image.open(image_path1).convert('RGB')\n",
    "        img2 = Image.open(image_path2).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening images: {e}\")\n",
    "        return None\n",
    "    img1 = lpips_transform(img1).unsqueeze(0).to(device)\n",
    "    img2 = lpips_transform(img2).unsqueeze(0).to(device)\n",
    "    # Scale images from [0, 1] to [-1, 1]\n",
    "    img1 = 2 * img1 - 1\n",
    "    img2 = 2 * img2 - 1\n",
    "    with torch.no_grad():\n",
    "        lpips_val = lpips_loss_fn(img1, img2)\n",
    "    return lpips_val.item()\n",
    "\n",
    "# Compute the average LPIPS score for a set of paired images.\n",
    "# Here, we assume a one-to-one correspondence (or you may randomly pair images).\n",
    "num_pairs = 400  # Change as needed.\n",
    "lpips_scores = []\n",
    "for i in range(num_pairs):\n",
    "    score = compute_lpips_score(palette_image_paths[i], sg2_image_paths[i])\n",
    "    if score is not None:\n",
    "        lpips_scores.append(score)\n",
    "\n",
    "if lpips_scores:\n",
    "    avg_lpips = np.mean(lpips_scores)\n",
    "    print(\"Average LPIPS between Palette and StyleGAN2:\", avg_lpips)\n",
    "else:\n",
    "    print(\"No valid LPIPS scores computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from scipy.linalg import sqrtm\n",
    "import lpips\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################################\n",
    "#                             FID IMPLEMENTATION                              #\n",
    "###############################################################################\n",
    "\n",
    "# Define a transformation for the Inception model:\n",
    "# Resize images to 299x299 and normalize according to Inception requirements.\n",
    "fid_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_activations(image_paths, model, batch_size=50):\n",
    "    \"\"\"\n",
    "    Extract activations (features) from the pre-trained model for a list of images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            images = []\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    img = Image.open(path).convert('RGB')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error opening {path}: {e}\")\n",
    "                    continue\n",
    "                img = fid_transform(img)\n",
    "                images.append(img)\n",
    "            if not images:\n",
    "                continue\n",
    "            images = torch.stack(images).to(device)\n",
    "            pred = model(images)\n",
    "            activations.append(pred.cpu().numpy())\n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    return activations\n",
    "\n",
    "# Load pre-trained InceptionV3 model and modify it to output features.\n",
    "inception = models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "# Remove the final fully connected layer by replacing it with an identity function.\n",
    "inception.fc = torch.nn.Identity()\n",
    "\n",
    "def calculate_fid(act1, act2):\n",
    "    \"\"\"\n",
    "    Calculate the Fréchet Inception Distance given two sets of activations.\n",
    "    \"\"\"\n",
    "    mu1 = np.mean(act1, axis=0)\n",
    "    mu2 = np.mean(act2, axis=0)\n",
    "    sigma1 = np.cov(act1, rowvar=False)\n",
    "    sigma2 = np.cov(act2, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Example: Get image paths from your DataFrames (adjust based on your code)\n",
    "# Assume palette_sample and sg2_sample have an 'image_path' column.\n",
    "path_gan1 = './data/BEST/everything/big_gan/metadata.csv'\n",
    "path_d1 = './data/BEST/everything/vq_diffusion/metadata.csv'\n",
    "big_gan_e = pd.read_csv(path_gan1)\n",
    "vq_diffusion_e = pd.read_csv(path_d1)\n",
    "\n",
    "big_gan_e['image_path']      = './data/BEST/everything/big_gan/' + big_gan_e['image_path']\n",
    "vq_diffusion_e['image_path'] = './data/BEST/everything/vq_diffusion/' + vq_diffusion_e['image_path']\n",
    "\n",
    "big_gan_image_paths = big_gan_e['image_path'].tolist()\n",
    "vqd_image_paths     = vq_diffusion_e['image_path'].tolist()\n",
    "\n",
    "# Extract activations for both sets.\n",
    "activations_bg  = get_activations(big_gan_image_paths, inception)\n",
    "activations_vqd = get_activations(vqd_image_paths, inception)\n",
    "\n",
    "# Compute the FID score.\n",
    "fid_value = calculate_fid(activations_vqd, activations_bg)\n",
    "print(\"FID between diffusion_vq and Big Gan:\", fid_value)\n",
    "\n",
    "###############################################################################\n",
    "#                        LPIPS (PERCEPTUAL METRIC)                            #\n",
    "###############################################################################\n",
    "\n",
    "# Initialize the LPIPS model. You can choose 'alex' or 'vgg' as the backbone.\n",
    "lpips_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "# Define a transform for LPIPS.\n",
    "lpips_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # LPIPS typically works on 256x256 images.\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def compute_lpips_score(image_path1, image_path2):\n",
    "    \"\"\"\n",
    "    Compute the LPIPS score between two images.\n",
    "    LPIPS expects images in the range [-1, 1], so we scale them accordingly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img1 = Image.open(image_path1).convert('RGB')\n",
    "        img2 = Image.open(image_path2).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening images: {e}\")\n",
    "        return None\n",
    "    img1 = lpips_transform(img1).unsqueeze(0).to(device)\n",
    "    img2 = lpips_transform(img2).unsqueeze(0).to(device)\n",
    "    # Scale images from [0, 1] to [-1, 1]\n",
    "    img1 = 2 * img1 - 1\n",
    "    img2 = 2 * img2 - 1\n",
    "    with torch.no_grad():\n",
    "        lpips_val = lpips_loss_fn(img1, img2)\n",
    "    return lpips_val.item()\n",
    "\n",
    "# Compute the average LPIPS score for a set of paired images.\n",
    "# Here, we assume a one-to-one correspondence (or you may randomly pair images).\n",
    "num_pairs = 400  # Change as needed.\n",
    "lpips_scores = []\n",
    "for i in range(num_pairs):\n",
    "    score = compute_lpips_score(big_gan_image_paths[i], vqd_image_paths[i])\n",
    "    if score is not None:\n",
    "        lpips_scores.append(score)\n",
    "\n",
    "if lpips_scores:\n",
    "    avg_lpips = np.mean(lpips_scores)\n",
    "    print(\"Average LPIPS between diffusion_vq and Big Gan:\", avg_lpips)\n",
    "else:\n",
    "    print(\"No valid LPIPS scores computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from scipy.linalg import sqrtm\n",
    "import lpips\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "###############################################################################\n",
    "#                             FID IMPLEMENTATION                              #\n",
    "###############################################################################\n",
    "\n",
    "# Define a transformation for the Inception model:\n",
    "# Resize images to 299x299 and normalize according to Inception requirements.\n",
    "fid_transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_activations(image_paths, model, batch_size=50):\n",
    "    \"\"\"\n",
    "    Extract activations (features) from the pre-trained model for a list of images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            images = []\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    img = Image.open(path).convert('RGB')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error opening {path}: {e}\")\n",
    "                    continue\n",
    "                img = fid_transform(img)\n",
    "                images.append(img)\n",
    "            if not images:\n",
    "                continue\n",
    "            images = torch.stack(images).to(device)\n",
    "            pred = model(images)\n",
    "            activations.append(pred.cpu().numpy())\n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    return activations\n",
    "\n",
    "# Load pre-trained InceptionV3 model and modify it to output features.\n",
    "inception = models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "# Remove the final fully connected layer by replacing it with an identity function.\n",
    "inception.fc = torch.nn.Identity()\n",
    "\n",
    "def calculate_fid(act1, act2):\n",
    "    \"\"\"\n",
    "    Calculate the Fréchet Inception Distance given two sets of activations.\n",
    "    \"\"\"\n",
    "    mu1 = np.mean(act1, axis=0)\n",
    "    mu2 = np.mean(act2, axis=0)\n",
    "    sigma1 = np.cov(act1, rowvar=False)\n",
    "    sigma2 = np.cov(act2, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "# Example: Get image paths from your DataFrames (adjust based on your code)\n",
    "# Assume palette_sample and sg2_sample have an 'image_path' column.\n",
    "path_gan2 = './data/BEST/bedroom_comparison/gansformer/metadata.csv'\n",
    "path_d2 = './data/BEST/bedroom_comparison/ddpm/metadata.csv'\n",
    "\n",
    "gansformer_b = pd.read_csv(path_gan2)\n",
    "ddpm_b = pd.read_csv(path_d2)\n",
    "\n",
    "gansformer_b['image_path'] = './data/BEST/bedroom_comparison/gansformer/' + gansformer_b['image_path']\n",
    "ddpm_b['image_path']       = './data/BEST/bedroom_comparison/ddpm/' + ddpm_b['image_path']\n",
    "\n",
    "gansformer_image_paths = gansformer_b['image_path'].tolist()\n",
    "ddpm_image_paths       = ddpm_b['image_path'].tolist()\n",
    "\n",
    "# Extract activations for both sets.\n",
    "activations_g  = get_activations(gansformer_image_paths, inception)\n",
    "activations_ddpm = get_activations(ddpm_image_paths, inception)\n",
    "\n",
    "# Compute the FID score.\n",
    "fid_value = calculate_fid(activations_ddpm, activations_g)\n",
    "print(\"FID between DDPM and Gansformer:\", fid_value)\n",
    "\n",
    "###############################################################################\n",
    "#                        LPIPS (PERCEPTUAL METRIC)                            #\n",
    "###############################################################################\n",
    "\n",
    "# Initialize the LPIPS model. You can choose 'alex' or 'vgg' as the backbone.\n",
    "lpips_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "# Define a transform for LPIPS.\n",
    "lpips_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # LPIPS typically works on 256x256 images.\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def compute_lpips_score(image_path1, image_path2):\n",
    "    \"\"\"\n",
    "    Compute the LPIPS score between two images.\n",
    "    LPIPS expects images in the range [-1, 1], so we scale them accordingly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img1 = Image.open(image_path1).convert('RGB')\n",
    "        img2 = Image.open(image_path2).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening images: {e}\")\n",
    "        return None\n",
    "    img1 = lpips_transform(img1).unsqueeze(0).to(device)\n",
    "    img2 = lpips_transform(img2).unsqueeze(0).to(device)\n",
    "    # Scale images from [0, 1] to [-1, 1]\n",
    "    img1 = 2 * img1 - 1\n",
    "    img2 = 2 * img2 - 1\n",
    "    with torch.no_grad():\n",
    "        lpips_val = lpips_loss_fn(img1, img2)\n",
    "    return lpips_val.item()\n",
    "\n",
    "# Compute the average LPIPS score for a set of paired images.\n",
    "# Here, we assume a one-to-one correspondence (or you may randomly pair images).\n",
    "num_pairs = 400  # Change as needed.\n",
    "lpips_scores = []\n",
    "for i in range(num_pairs):\n",
    "    score = compute_lpips_score(gansformer_image_paths[i], ddpm_image_paths[i])\n",
    "    if score is not None:\n",
    "        lpips_scores.append(score)\n",
    "\n",
    "if lpips_scores:\n",
    "    avg_lpips = np.mean(lpips_scores)\n",
    "    print(\"Average LPIPS between DDPM and Gansformer:\", avg_lpips)\n",
    "else:\n",
    "    print(\"No valid LPIPS scores computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Algorithm section (C1), you should explain the principles behind deep neural networks, the structure of U-Net, the purpose of skip connections, and the mathematical definition of cross-entropy loss, etc.\n",
    "\n",
    "In the Network Design section (C2), you can discuss how many layers you used to build your U-Net, how you defined the weights for the weighted CE loss, and other design choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review diagrams (dimensions etc)\n",
    "Add mathematical definition (cross-entropy etc)\n",
    "Purpose of skip connections\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipemag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
